<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fseast Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-10T18:02:34.163Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>fseast</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>（一）深入学习MapReduce——MapReduce概述与Hadoop序列化</title>
    <link href="http://yoursite.com/2018/03/09/buy-%EF%BC%88%E4%B8%80%EF%BC%89%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0MapReduce%E2%80%94%E2%80%94MapReduce%E6%A6%82%E8%BF%B0%E4%B8%8EHadoop%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <id>http://yoursite.com/2018/03/09/buy-（一）深入学习MapReduce——MapReduce概述与Hadoop序列化/</id>
    <published>2018-03-09T04:23:29.000Z</published>
    <updated>2019-09-10T18:02:34.163Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、MapReduce概述"><a href="#一、MapReduce概述" class="headerlink" title="一、MapReduce概述"></a>一、MapReduce概述</h2><h3 id="1-MapReduce定义"><a href="#1-MapReduce定义" class="headerlink" title="1.MapReduce定义"></a>1.MapReduce定义</h3><p>MapReduce 是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。</p><p>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并运行在一个Hadoop集群上。</p><h3 id="2-MapReduce优缺点"><a href="#2-MapReduce优缺点" class="headerlink" title="2.MapReduce优缺点"></a>2.MapReduce优缺点</h3><h4 id="2-1优点"><a href="#2-1优点" class="headerlink" title="2.1优点"></a>2.1优点</h4><ol><li>MapReduce易于编程<br>它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。这个特点是MapReduce编程变得非常流行。</li><li>良好的扩展性<br>当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。</li><li>高容错性<br>MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而这个过程不需要人工参与，而是完全有Hadoop内部完成。</li><li>适合PB级以上海量数据的离线处理</li><li>可以实现上千台服务器集群并发工作，提高数据处理能力。</li></ol><h4 id="2-2缺点"><a href="#2-2缺点" class="headerlink" title="2.2缺点"></a>2.2缺点</h4><ol><li>不擅长实时计算<br>MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。</li><li>不擅长流式计算<br>流式计算的话输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。</li><li>不擅长有向图（DAG）计算<br>多个应用程序存在依赖关系，后一个程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写到磁盘，会造成大量的磁盘IO，导致性能非常的低下。</li></ol><h3 id="3-MapReduce核心思想"><a href="#3-MapReduce核心思想" class="headerlink" title="3.MapReduce核心思想"></a>3.MapReduce核心思想</h3><p>1）分布式的运行程序往往需要分成两个阶段<br>（1）第一个阶段的MapTask并发实例，完全并行运行，互不相干。<br>（2）第二个阶段的ReduceTask并发实例，互不相干，但是他们的数据依赖上一个阶段的所有MapTask并发实力的输出。<br>2）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。</p><p><img src="https://img-blog.csdnimg.cn/20190725153348577.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="MapReduce核心思想"><br>对上图进行分析：<br>需求是统计每个单词出现的次数，a-p开头的单词输出结果到一个文件中，q - z开头的单词输出到一个文件中。<br>（1）多个MapTask并发执行，每个Task负责一部分的单词处理。Map阶段的每个Task对文件进行读数据，一般是按行获取，然后把一行单词切割成多个单词，然后得到结果为（单词，1）的键值对的方式，还要对结果根据单词首字母进行判断，分成两个区。<br>（2）Reduce阶段通过Map的输出结果汇总。Map阶段的输出就是Reduce阶段的输入，也就是Reduce拿到（单词，1）这样的格式的输入，通过相同的key也就是单词，进行value的累加，就可以得到每个单词的总数，比如Map输出的结果有（hadoop，1），（hadoop，1），（hadoop，1），然后Reduce阶段判断hadoop都是相同的key，对value进行累加，最后输出的结果就是（hadoop，3）在写到文件中。</p><h3 id="4-MapReduce进程"><a href="#4-MapReduce进程" class="headerlink" title="4.MapReduce进程"></a>4.MapReduce进程</h3><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p><ol><li>MrAppMaster：负责整个程序的过程调度及状态协调。</li><li>MapTask：负责Map阶段的整个数据处理流程。</li><li>ReduceTask：负责Reduce阶段的整个数据处理流程。</li></ol><h3 id="5-常用数据序列化类型"><a href="#5-常用数据序列化类型" class="headerlink" title="5.常用数据序列化类型"></a>5.常用数据序列化类型</h3><p>常用的数据类型对应Hadoop数据序列化类型：<br><img src="https://img-blog.csdnimg.cn/20190725155853538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="Hadoop数据序列化类型"></p><h3 id="6-MapReduce编程规范"><a href="#6-MapReduce编程规范" class="headerlink" title="6.MapReduce编程规范"></a>6.MapReduce编程规范</h3><p>用户编写的程序分成三个部分：Mapper、Reduce 和 Driver</p><ol><li><p>Mapper阶段<br>（1）用户自定义的Mapper要继承自己的父类<br>（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）<br>（3）Mapper中的业务逻辑写在map（）方法中<br>（4）Mapper的输出数据是KV对的形式（KV的类型可自定义）<br>（5）map（）方法（MapTask进程）对每一个&lt;K,V&gt;调用一次。</p></li><li><p>Reducer阶段<br>（1）用户自定义的Reducer要继承自己的父类<br>（2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV<br>（3）Reducer的业务逻辑写在reduce（）方法中<br>（4）ReduceTask进程对每一组相同&lt;k,v&gt;组调用一次reduce（）方法。</p></li><li><p>Driver阶段<br>相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装MapReduce程序相关运行参数的job对象。</p></li></ol><h3 id="7-WordCount案例实操"><a href="#7-WordCount案例实操" class="headerlink" title="7.WordCount案例实操"></a>7.WordCount案例实操</h3><p>需求：<br>在给定的文本文件中统计输出每一个单词出现的总次数。</p><p>单词准备：<br><img src="https://img-blog.csdnimg.cn/20190725212655215.png" alt="在这里插入图片描述"></p><ol><li>创建Maven工程，并添加依赖：</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li><p>在项目的src/main/resources下创建名为log4j.properties的文件，并在文件中添加：<br>（作用：可以查看代码执行后产生的日志）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout</span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n</span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender</span><br><span class="line">log4j.appender.logfile.File=target/spring.log</span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure></li><li><p>编写程序：<br>（1）编写Mapper类：</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Map阶段</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 需要继承Mapper，并重写map方法，完成自定义的Mapper，</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * KEYIN:输入的Key的类型</span></span><br><span class="line"><span class="comment"> * VALUEIN：输入的Value的类型</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * KEYOUT：输出的Key的类型</span></span><br><span class="line"><span class="comment"> * VALUEOUT：输出的Value的类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">      Text k = <span class="keyword">new</span> Text();</span><br><span class="line">      IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 获取一行数据，例如： hadoop hadoop</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 切割数据</span></span><br><span class="line">        String[] split = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 写出数据</span></span><br><span class="line">        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">            k.set(word);</span><br><span class="line">            System.out.println(<span class="string">"k=====："</span> + k + <span class="string">"v========="</span> + v);</span><br><span class="line">            context.write(k,v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）编写Reduce类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Reduce 阶段</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 通过继承Reduce，重写 reduce方法，完成自定义的Reducer</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 将相同key的value进行汇总。</span></span><br><span class="line">            <span class="comment">//hadoop 1</span></span><br><span class="line">            <span class="comment">//hadoop 1</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        v.set(sum);</span><br><span class="line">        <span class="comment">//2. 写出</span></span><br><span class="line">        context.write(key,v);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（3）编写Driver驱动类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//1. 获取Job对象 ==&gt; 一个MapReduce程序实际上就是一个job</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 关联jar</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">//3.关联当前Job对应的Mapper 和 Reducer</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//4. 设置Mapper 输出的key 和 value的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置最终输出的key 和 value的类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 设置文件的输入 和 结果的输出位置</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">"E:/file/test/intout/inputWord/word"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"E:/file/test/intout/inputWord/word/outputWord"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7. 提交job</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>本地测试：<br>（1）这里使用的win10的电脑，将win10的hadoop jar包解压到非中文路径，并在Windows环境上配置HADOOP_HOME 环境变量。<br><img src="https://img-blog.csdnimg.cn/20190725211307709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190725211354562.png" alt="在这里插入图片描述"></li></ol><p>Path：<br><img src="https://img-blog.csdnimg.cn/2019072521142734.png" alt="在这里插入图片描述"></p><p>（2）运行程序：<br>日志部分截图：<br><img src="https://img-blog.csdnimg.cn/20190725212908903.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">结果：<br><img src="https://img-blog.csdnimg.cn/20190725212940491.png" alt="在这里插入图片描述"><br>打开文件：<br><img src="https://img-blog.csdnimg.cn/20190725213009661.png" alt="在这里插入图片描述"></p><ol><li>集群上测试<br>（1）用maven打jar包，需要添加的打包插件依赖。<br>有一行需要改成自己主类工程</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin <span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.fseast.mr.wordcount.WordCountDriver<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span>  <span class="comment">&lt;!-- 此处需修改 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（2）将程序打包成jar包，然后拷贝到Hadoop集群中。（为了方便操作，把名字修改为wc.jar）<br>（3）启动hadoop集群<br>（4）执行WordCount程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar  wc.jar com.fseast.mr.wordcount.WordcountDriver /user/fseast/input /user/fseast/output</span><br></pre></td></tr></table></figure><h2 id="二、Hadoop序列化"><a href="#二、Hadoop序列化" class="headerlink" title="二、Hadoop序列化"></a>二、Hadoop序列化</h2><h3 id="1-序列化概述"><a href="#1-序列化概述" class="headerlink" title="1.序列化概述"></a>1.序列化概述</h3><h4 id="1-1什么是序列化"><a href="#1-1什么是序列化" class="headerlink" title="1.1什么是序列化"></a>1.1什么是序列化</h4><p><strong>序列化</strong>就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便存储到磁盘（持久化）和网络传输。<br><strong>反序列化</strong>就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</p><h4 id="1-2为什么要序列化"><a href="#1-2为什么要序列化" class="headerlink" title="1.2为什么要序列化"></a>1.2为什么要序列化</h4><p>一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能有本地的进程使用，不能被发送到网络上的另一台计算机。然后序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。</p><h4 id="1-3为什么不使用Java的序列化"><a href="#1-3为什么不使用Java的序列化" class="headerlink" title="1.3为什么不使用Java的序列化"></a>1.3为什么不使用Java的序列化</h4><p>Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）</p><h4 id="1-4Hadoop序列化特点"><a href="#1-4Hadoop序列化特点" class="headerlink" title="1.4Hadoop序列化特点"></a>1.4Hadoop序列化特点</h4><ol><li>紧凑：高效实用存储空间。</li><li>快速： 读写数据的额外开销小。</li><li>可扩展：随着通信协议的升级而可升级。</li><li>互操作：支持多语言的交互。</li></ol><h3 id="2-自定义bean对象实现序列化接口（Writable）"><a href="#2-自定义bean对象实现序列化接口（Writable）" class="headerlink" title="2.自定义bean对象实现序列化接口（Writable）"></a>2.自定义bean对象实现序列化接口（Writable）</h3><p>实际开发过程中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</p><p>具体实现bean对象序列化步骤有如下7步：<br>（1）必须实现Writable接口<br>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（3）重写序列化方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeLong(upFlow);</span><br><span class="line">out.writeLong(downFlow);</span><br><span class="line">out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（4）重写反序列化方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">upFlow = in.readLong();</span><br><span class="line">downFlow = in.readLong();</span><br><span class="line">sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（5）注意反序列化的顺序和序列化的顺序完全一致。<br>（6）要想把结果显示在文件中，需要重写toString，可用“\t”分开，方便后续切割。<br>（7）如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 倒序排列，从大到小</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-序列化案例实操"><a href="#3-序列化案例实操" class="headerlink" title="3.序列化案例实操"></a>3.序列化案例实操</h3><ol><li>需求<br>统计每个手机号耗费的总上行流量、下行流量、总流量<br>（1）原始数据如图：<br><img src="https://img-blog.csdnimg.cn/20190822154821434.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>（2）输入数据格式：<br><img src="https://img-blog.csdnimg.cn/20190725233700133.png" alt="在这里插入图片描述"><br>（3）要求输出数据格式：<br><img src="https://img-blog.csdnimg.cn/20190725233802315.png" alt="在这里插入图片描述"></li><li>需求分析<br>Map阶段：<br>（1）读取一行数据，切分字段<br>（2）从字段中抽取出手机号、上行流量、下行流量。<br>（3）以手机号为key，bean对象为value输出。<br>（4） ==bean对象要想能够传输，必须实现序列化接口==</li></ol><p>Reduce阶段：<br>累加上行流量和下行流量得到总流量。</p><ol><li>编写MapReduce程序<br>（1）编写流量统计的Bean对象</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 描述 上行流量 下行流量 总流量</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 因为要写到磁盘，因此该类要实现Hadoop的序列化接口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 反序列化时，需要反射调用空参构造器，所以必须有</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 序列化方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 反序列化方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 注意：反序列化的顺序要与序列化的顺序一致。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        upFlow = dataInput.readLong();</span><br><span class="line">        downFlow = dataInput.readLong();</span><br><span class="line">        sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> dowanFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dowanFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//编写toString方法，方便后续切割或使用</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//为了方便统计总流量</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow,<span class="keyword">long</span> dowanFlow)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dowanFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = upFlow + dowanFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）编写Mapper类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    Text outK = <span class="keyword">new</span> Text();</span><br><span class="line">    FlowBean outV = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 获取一行， </span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//2. 切割字段</span></span><br><span class="line">        String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//处理数据</span></span><br><span class="line">        <span class="comment">//取出手机号</span></span><br><span class="line">        String k = split[<span class="number">1</span>];</span><br><span class="line">        outK.set(k);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//取出上行流量和下行流量</span></span><br><span class="line">        String upFlow = split[split.length - <span class="number">3</span>];</span><br><span class="line">        String downFlow = split[split.length - <span class="number">2</span>];</span><br><span class="line">        outV.set(Long.parseLong(upFlow),Long.parseLong(downFlow));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        context.write(outK,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（3）编写Reducer类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    FlowBean v = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//汇总</span></span><br><span class="line">        <span class="keyword">int</span> totalUpFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> totalDownFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> totalSumFlow = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历所有bean，将其中的上行流量，下行流量，总流量分别累加</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean : values) &#123;</span><br><span class="line">            totalUpFlow += flowBean.getUpFlow();</span><br><span class="line">            totalDownFlow += flowBean.getDownFlow();</span><br><span class="line">            totalSumFlow += flowBean.getSumFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//封装对象</span></span><br><span class="line">        v.setUpFlow(totalUpFlow);</span><br><span class="line">        v.setDownFlow(totalDownFlow);</span><br><span class="line">        v.setSumFlow(totalSumFlow);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        context.write(key, v);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（4）编写Driver驱动类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        args = <span class="keyword">new</span> String[] &#123;<span class="string">"e:/file/test/intout/inputWord"</span>,<span class="string">"e:/file/test/intout/output"</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 获取Job</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 指定本程序的jar包所在的本地路径</span></span><br><span class="line">        job.setJarByClass(FlowCountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 关联Mapper 和 Reducer</span></span><br><span class="line">        job.setMapperClass(FlowCountMapper.class);</span><br><span class="line">        job.setReducerClass(FlowCountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置Map输出的key和value的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置最终输出的key和value的类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7. 提交Job中配置的相关参数等</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（5）运行：<br>部分运行结果截图：<br><img src="https://img-blog.csdnimg.cn/2019072618373282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190726183755476.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190726183844189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、MapReduce概述&quot;&gt;&lt;a href=&quot;#一、MapReduce概述&quot; class=&quot;headerlink&quot; title=&quot;一、MapReduce概述&quot;&gt;&lt;/a&gt;一、MapReduce概述&lt;/h2&gt;&lt;h3 id=&quot;1-MapReduce定义&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="MapReduce" scheme="http://yoursite.com/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>（二）HDFS——节点分析及新特性</title>
    <link href="http://yoursite.com/2018/03/07/buy-%EF%BC%88%E4%BA%8C%EF%BC%89HDFS%E2%80%94%E2%80%94%E8%8A%82%E7%82%B9%E5%88%86%E6%9E%90%E5%8F%8A%E6%96%B0%E7%89%B9%E6%80%A7/"/>
    <id>http://yoursite.com/2018/03/07/buy-（二）HDFS——节点分析及新特性/</id>
    <published>2018-03-07T14:18:51.000Z</published>
    <updated>2019-09-11T00:35:02.190Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、NameNode和SecondaryNameNode"><a href="#一、NameNode和SecondaryNameNode" class="headerlink" title="一、NameNode和SecondaryNameNode"></a>一、NameNode和SecondaryNameNode</h2><h3 id="1-NN和2NN工作机制"><a href="#1-NN和2NN工作机制" class="headerlink" title="1.NN和2NN工作机制"></a>1.NN和2NN工作机制</h3><font color="#4169E1" size="3">NameNode中的元数据是存储在哪里的？</font><br>&#8195;&#8195;首先，假设存储在NameNode节点中的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。<br>这样又会带来新的问题，当在内存中的原始数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生不一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage 和Edits的合并，合成元数据。<br>&#8195;&#8195;但是，如果长时间添加数据到Edits中，会导致文该文件过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作有NameNode节点完成，又会效率过低。因此，引入一个新节点SecondaryNameNode（简称2NN），专门用于FsImage和Edits的合并。<br><br>NN和2NN工作机制如图所示：<br><img src="https://img-blog.csdnimg.cn/20190730185912481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>1. 第一个阶段：NameNode启动<br>（1）第一次启动NameNode格式化后，创建Fsimage和 Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>（2）客户端对元数据进行增删改的请求。<br>（3）NameNode记录操作日志，更新滚动日志。<br>（4）NameNode在内存中对元数据进行增删改。<br>2. 第二阶段：SecondaryNameNode工作<br>（1）SecondaryNameNode询问NameNode是否需要CheckPonit。直接带回NameNode是否检查结果。<br>（2）SecondaryNameNode请求执行CheckPoint。<br>（3）NameNode滚动 正在写的Edits日志。<br>（4）将滚动前的编辑日志和镜像文件拷贝到SecondaryNameNode。<br>（5）SecondaryNameNode加载编辑日志和镜像文件到内存，并合并。<br>（6）生成新的镜像文件fsimage.chkpoint。<br>（7）拷贝fsimage.chkploint 到 NameNode。<br>（8）NameNode 将 fsimage.chkpoint 重新命名成fsimage。<br><br><br>&gt; NN和2NN工作机制详解：<br>&gt; Fsimage：NameNode内存中元数据序列化后形成的文件。<br>&gt; Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。<br>&gt; NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。<br>&gt; 由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。<br>&gt; SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。<br><br>### 2.Fsimage 和 Edits解析<br><font color="#4169E1" size="3">1. 概念</font><br>&#8195;&#8195;NameNode背个时候，将在/opt/moudule/hadoop-2.7.2/data/tmp/dfs/name/current 目录中产生如下文件。<br><img src="https://img-blog.csdnimg.cn/2019073019211112.png" alt="在这里插入图片描述">（1）Fsimage 文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。<br>（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。<br>（3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字。<br>（4）每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候将Fsimage和Edits文件进行了合并。<br><br><font color="#4169E1" size="3">2. oiv查看Fsimage文件</font><br>（1）查看oiv 和 oev命令<br><br>（2）基本语法<br>hdfs oiv -p 文件类型 -i 镜像文件 -o  转换后文件输出路径<br><br>（3）案例实操<br><br><font color="#4169E1" size="3">3. oev查看Edits文件</font><br>（1）基本语法<br>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径<br><br>（2）案例实操<br><br><br>###  3.CheckPoint时间设置<br>（1）通常情况下，SecondaryNameNode每隔一小时执行一次。<br>在默认配置文件里：<br>【hdfs-default.xml】<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br>（2）一分钟检查一次操作次数，当操作次数达到一百万时，SecondaryNameNode执行一次。<br><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure><br><br>### 4.NameNode故障处理<br>NameNode故障后，可以采用如下两种方法恢复数据。<br><font color="#4169E1" size="3">方法一：</font>将secondaryNameNode中数据拷贝到NameNode存储数据的目录。<br>（1）kill -9 NameNode 进程<br>（2）删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure><br><br>（3）拷贝SecondaryNameNode中数据到远NameNode存储数据目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 dfs]$ scp -r fseast@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure><br><br>（4）重新启动NameNode<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><br><br><font color="#4169E1" size="3">方法二：</font>使用-importCheckpoint 选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。<br>（1）修改hdfs-site.xml中的<br><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br>（2）kill -9 NameNode进程<br>（3）删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure><br><br>（4）如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件。<br>（5）导入检查点数据（等待一会Ctrl+c结束掉）<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure><br><br>（6）启动NameNode<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><br><br>### 5.集群安全模式<br>1. 概述<br>（1）NameNode启动<br>&#8195;&#8195;NameNode启动时，首先将镜像文件（Fsimage）载入内存，并执行编辑日志（Edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的。<br>（2）DataNode启动<br>&#8195;&#8195;系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。<br>（3）安全模式退出判断<br>&#8195;&#8195;如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。<br><br>2. 基本语法<br>&#8195;&#8195;集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。<br>    （1）bin/hdfs dfsadmin -safemode get        （功能描述：查看安全模式状态）<br>    （2）bin/hdfs dfsadmin -safemode enter      （功能描述：进入安全模式状态）<br>    （3）bin/hdfs dfsadmin -safemode leave    （功能描述：离开安全模式状态）<br>    （4）bin/hdfs dfsadmin -safemode wait    （功能描述：等待安全模式状态）<br><br>### 6.NameNode多目录配置<br>1. NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性<br>2. 具体配置如下：<br>（1）在hdfs-site.xml文件中增加如下内容<br><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br>（2）停止集群，删除 data 和 logs 中所有数据。<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/</span><br><span class="line">[fseast@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/</span><br><span class="line">[fseast@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure><br><br>（3）格式化集群并启动<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format</span><br><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><br><br>（4）查看结果<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 fseast fseast 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 fseast fseast 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 fseast fseast 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure><br><br>## 二、DataNode<br>### 1.DataNode工作机制<br>DataNode工作机制如图所示：<br><img src="https://img-blog.csdnimg.cn/2019073020305575.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="DataNode工作机制">（1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个数据本身，一个是元数据包括数据库的长度，块数据的校验和，以及时间戳。<br>（2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。<br>（3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或者删除某一个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。<br>（4）集群运行中可以安全加入和退出一些机器。<br><br>### 2.数据完整性<br>【思考】如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（2），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如果解决呢？<br>&#8195;&#8195;如下是DataNode节点保证数据完整性的方法：<br>（1）当DataNode读取Block的时候，它会计算CheckSum。<br>（2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。<br>（3）Client读取其他DataNode上的Block。<br>（4）DataNode在其他文件创建后周期验证CheckSum，如图所示：<br><img src="https://img-blog.csdnimg.cn/20190730204550312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>### 3.掉线时限参数设置<br><br>1. DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信。<br>2. NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。<br>3. HDFS默认的超时时长为10分钟+30秒。<br>4. 如果定义超时时间为TimeOut，则超时时长的计算公式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TimeOut  = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。</span><br></pre></td></tr></table></figure><br><br>而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。<br><br>&#8195;&#8195;需要注意的是 hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval的单位为秒。<br><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br>### 4.服役新数据节点<br>需求：<br>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。<br>1. 环境准备：<br>（1）准备新的节点hadoop105，修改IP地址和主机名<br>（2）把hadoop104用到的相关软件及配置信息拷贝过去。<br>（3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）<br>（4）source 一下配置文件 <code>source /etc/profile</code><br><br>2. 服役新节点具体步骤<br>（1）直接启动DataNode，即关联集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop105 hadoop-2.7.2]$ hadoop-daemon.sh start datanode</span><br><span class="line">[fseast@hadoop105 hadoop-2.7.2]$ yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><br><br>（2）在hadoop105上上传文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop105 hadoop-2.7.2]$ hadoop fs -put /opt/module/hadoop-2.7.2/LICENSE.txt /</span><br></pre></td></tr></table></figure><br><br>（3）如果数据不均衡，可以用命令实现集群的再平衡<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 sbin]$ ./start-balancer.sh</span><br><span class="line">starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-fseast-balancer-hadoop102.out</span><br><span class="line">Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span><br></pre></td></tr></table></figure><br><br>### 5.退役旧数据节点<br>#### 5.1添加白名单<br>&#8195;&#8195;添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。<br>配置白名单的具体步骤如下：（操作退出hadoop105）<br>（1）在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[fseast@hadoop102 hadoop]$ touch dfs.hosts</span><br><span class="line">[fseast@hadoop102 hadoop]$ vim dfs.hosts</span><br></pre></td></tr></table></figure><br><br>&#8195;&#8195;添加如下主机名称（不添加hadoop105）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><br><br>（2）在 NameNode 的 hdfs-site.xml 配置文件中增加 dfs.hosts 属性<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br>（3）配置文件分发到其他节点<br><br>（4）刷新NameNode<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br></pre></td></tr></table></figure><br><br>（5）更新ResourceManager节点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><br><br>（6）在web浏览器上查看<br><br><br>（7）如果数据不均衡，可以用命令实现集群的再平衡<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-balancer.sh</span><br></pre></td></tr></table></figure><br><br>#### 5.2黑名单退役<br>在黑名单上面的主机都会被强制退出。<br>1. 在NameNode 的 /opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件，并在该文件中添加如下主机名称（要退役的节点）：<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop105</span><br></pre></td></tr></table></figure><br><br>2. 在NameNode 的 hdfs-site.xml 配置文件中增加dfs.hosts.exclude 属性：<br><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br>3. 刷新NameNode、刷新ResourceManager<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</span><br></pre></td></tr></table></figure><br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><br><br>4. 检查Web浏览器，退役节点的状态为 Decommission In Progress（退役中），说明数据节点正在复制快到其他节点。<br><br>5. 等待退役节点状态为Decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3， 是不能退役成功的，需要修改副本数后才能退役。<br>停止该节点：<code>hadoop-daemon.sh stop datanode</code><br>停止该节点资源管理器：<code>yarn-daemon.sh stop nodemanager</code><br><br>6. 如果数据不均衡，可以用命令实现集群的再平衡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-balancer.sh</span><br></pre></td></tr></table></figure><br><br><font color="Crimson" size="3">注意：不允许白名单和黑名单中同时出现同一个主机名称</font><h3 id="6-DataNode多目录配置"><a href="#6-DataNode多目录配置" class="headerlink" title="6.DataNode多目录配置"></a>6.DataNode多目录配置</h3><ol><li>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本。（用处不大）</li><li>具体配置如下<br>hdfs-site.xml</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="三、HDFS-2-X-新特性"><a href="#三、HDFS-2-X-新特性" class="headerlink" title="三、HDFS 2.X 新特性"></a>三、HDFS 2.X 新特性</h2><h3 id="1-集群间数据拷贝"><a href="#1-集群间数据拷贝" class="headerlink" title="1.集群间数据拷贝"></a>1.集群间数据拷贝</h3><ol><li>scp 实现两个远程主机之间的文件复制</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scp -r hello.txt root@hadoop103:/user/fseast/hello.txt// 推 push</span><br><span class="line"></span><br><span class="line">scp -r root@hadoop103:/user/fseast/hello.txt  hello.txt// 拉 pull</span><br><span class="line"></span><br><span class="line">scp -r root@hadoop103:/user/fseast/hello.txt root@hadoop104:/user/fseast</span><br><span class="line">//是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。</span><br></pre></td></tr></table></figure><ol><li>采用 distcp 命令实现两个Hadoop 集群之间的递归数据复制。</li></ol><h3 id="2-小文件存档"><a href="#2-小文件存档" class="headerlink" title="2.小文件存档"></a>2.小文件存档</h3><ol><li>HDFS存储小文件弊端<br>&#8195;&#8195;每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置而为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。</li><li>解决存储小文件办法之一<br>&#8195;&#8195;HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体来说，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少NameNode的内存。<br><img src="https://img-blog.csdnimg.cn/20190801213522282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li><li>实例操作<br>（1）需要启动YARN进程<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure></li></ol><p>（2）归档文件<br>&#8195;&#8195;把/user/fseast/input 目录里面的所有文件归档成一个叫input.har 的归档文件，并把归档后文件存储到/user/fseast/input/output 路径下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop archive -archiveName input.har –p  /user/fseast/input   /user/fseast/output</span><br></pre></td></tr></table></figure><p>（3）查看归档</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -lsr /user/fseast/output/input.har</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -lsr har:///user/fseast/output/input.har</span><br></pre></td></tr></table></figure><p>（4）解归档文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp har:/// user/fseast/output/input.har/*    /user/fseast</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、NameNode和SecondaryNameNode&quot;&gt;&lt;a href=&quot;#一、NameNode和SecondaryNameNode&quot; class=&quot;headerlink&quot; title=&quot;一、NameNode和SecondaryNameNode&quot;&gt;&lt;/a&gt;一、
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>（一）HDFS的认识及使用Java对其的简单操作</title>
    <link href="http://yoursite.com/2018/02/25/buy-%EF%BC%88%E4%B8%80%EF%BC%89HDFS%E7%9A%84%E8%AE%A4%E8%AF%86%E5%8F%8A%E4%BD%BF%E7%94%A8Java%E5%AF%B9%E5%85%B6%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2018/02/25/buy-（一）HDFS的认识及使用Java对其的简单操作/</id>
    <published>2018-02-25T13:23:17.000Z</published>
    <updated>2019-09-10T18:03:53.594Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、HDFS概述"><a href="#一、HDFS概述" class="headerlink" title="一、HDFS概述"></a>一、HDFS概述</h2><p>HDFS(Hadoop distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<br>使用场景：适合一次写入，多次读写的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。</p><h3 id="1-优点"><a href="#1-优点" class="headerlink" title="1.优点"></a>1.优点</h3><ol><li>高容错性<br>（1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。<br>（2）某一个副本丢失以后，它可以自动恢复。</li><li>适合处理大数据<br>（1）数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；<br>（2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。</li><li>可构建在廉价机器上，通过多副本机制，提高可靠性。</li></ol><h3 id="2-缺点"><a href="#2-缺点" class="headerlink" title="2.缺点"></a>2.缺点</h3><ol><li>不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。</li><li>无法高效的对大量小文件进行存储。<br>（1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；<br>（2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。</li><li>不支持并发写入、文件随机修改。<br>（1）一个文件只能有一个写，不允许多个线程同时写；<br>（2）仅支持数据append（追加），不支持文件的随时修改。</li></ol><h3 id="3-HDFS组成架构"><a href="#3-HDFS组成架构" class="headerlink" title="3.HDFS组成架构"></a>3.HDFS组成架构</h3><p>先通过一张图了解一下：<br><img src="https://img-blog.csdnimg.cn/20190720215137559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li>NameNode（nn）：就是Master，它是一个主管、管理者。<br>（1）管理HDFS的名称空间；<br>（2）配置副本策略；<br>（3）管理数据库（Block）映射信息；<br>（4）处理客户端读写请求。</li><li>DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。<br>（1）存储实际的数据块；<br>（2）执行数据块的读/写操作。</li><li>Secondary NameNode：并非NameNode的热备份。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。<br>（1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode；<br>（2）在紧急情况下，可辅助恢复NameNode。</li></ol><h3 id="4-HDFS文件块大小"><a href="#4-HDFS文件块大小" class="headerlink" title="4.HDFS文件块大小"></a>4.HDFS文件块大小</h3><p>HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M。<br><img src="https://img-blog.csdnimg.cn/2019072022325670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="HDFS文件块大小"></p><p>【问题：】为什么块的大小不能设置太小，也不能设置太大？<br>（1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；<br>（2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始所需的时间。导致程序在处理这块数据时，会非常慢。</p><p>总结：HDFS块的大小设置主要取决于磁盘传输速率。</p><h2 id="二、HDFS的Shell操作"><a href="#二、HDFS的Shell操作" class="headerlink" title="二、HDFS的Shell操作"></a>二、HDFS的Shell操作</h2><ol><li>基本语法：（有两个）</li></ol><p><code>bin/hadoop fs  具体命令</code> 或者 <code>bin/hdfs dfs 具体命令</code><br>dfs 是 fs 的实现类。</p><ol><li>常用命令实操：<br>启动Hadoop集群：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ sbin/ start-dfs.sh</span><br><span class="line">[fseast@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li></ol><p>（1）-help：输出这个命令参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -help rm</span><br></pre></td></tr></table></figure></p><p>（2）-ls：显示目录信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -ls /</span><br></pre></td></tr></table></figure></p><p>（3）-mkdir：在HDFS上创建目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -mkdir -p /user/fseast/</span><br></pre></td></tr></table></figure><p>（4）-moveFromLocal：从本地剪切粘贴到HDFS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ vim testmoveFrom.txt </span><br><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -moveFromLocal ./testmoveFrom.txt /user/fseast/</span><br></pre></td></tr></table></figure><p>（5）-appendToFile：追加一个文件到已经存在的文件末尾</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -appendToFile NOTICE.txt /user/fseast/testmoveFrom.txt</span><br></pre></td></tr></table></figure><p>（6）-cat：显示文件内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -cat /user/fseast/testmoveFrom.txt</span><br></pre></td></tr></table></figure></p><p>（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</p><p>（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyFromLocal test001.txt /</span><br></pre></td></tr></table></figure></p><p>（9）-copyToLocal：从HDFS拷贝到本地</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ rm -rf test001.txt </span><br><span class="line">[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyToLocal /test001.txt</span><br></pre></td></tr></table></figure><p>（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</p><p>（11）-mv：在HDFS目录中移动文件</p><p>（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地</p><p>（13）-getmerge：合并下载多个文件，比如HDFS的目录/user/fseast/test 下有多个文件：log.1 , log.2 , log.3 …</p><p>（14）-put：等同于copyFromLocal</p><p>（15）-tail：显示一个文件的末尾</p><p>（16）-rm：删除文件或文件夹</p><p>（17）-rmdir：删除空目录</p><p>（18）-du统计文件夹的大小信息</p><p>（19）-setrep：设置HDFS中文件的副本数量</p><h2 id="三、HDFS客户端操作"><a href="#三、HDFS客户端操作" class="headerlink" title="三、HDFS客户端操作"></a>三、HDFS客户端操作</h2><h3 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1.环境准备"></a>1.环境准备</h3><ol><li>创建Maven工程并导入相应得的依赖坐标     </li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li>需要在项目的src/main/resources目录下，新建一个文件，命名为log4j.properties，并在文件中填入（为了查看日志）：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout</span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n</span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender</span><br><span class="line">log4j.appender.logfile.File=target/spring.log</span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure><ol><li>编写一个类，测试是否连接成功</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 连通客户端与HDFS的连接</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="meta">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testClientConnectHDFS</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">     FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), conf, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     <span class="comment">//在HDFS上创建一个目录</span></span><br><span class="line">     fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/HdfsDemo02/test01"</span>));</span><br><span class="line"></span><br><span class="line">     <span class="comment">//关闭资源</span></span><br><span class="line">     fs.close();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="2-HDFS的API操作"><a href="#2-HDFS的API操作" class="headerlink" title="2.HDFS的API操作"></a>2.HDFS的API操作</h3><ol><li>HDFS文件上传（参数优先级）</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 测试1：上传文件到HDFS</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="meta">@Test</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocal</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="comment">//1.获取FileSystem对象</span></span><br><span class="line">      Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">      <span class="comment">//可以设置多少个副本，优先级最高</span></span><br><span class="line">      configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"2"</span>);</span><br><span class="line">      FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">//2.操作</span></span><br><span class="line">      fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"e:/file/test/fsdong.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/HdfsDemo02/test01"</span>));</span><br><span class="line"></span><br><span class="line">      <span class="comment">//3. 关闭资源</span></span><br><span class="line">      fileSystem.close();</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>可以将hdfs-site.xml拷贝到项目的根目录下。<br><strong>参数优先级：</strong><br>参数优先级排序：<br>（1）客户端代码中设置的值 &gt; （2）ClassPath 下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置</p><ol><li>HDFS文件下载</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 执行下载操作</span></span><br><span class="line"><span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line"><span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line"><span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line"><span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/HdfsDemo02/test01/wc.input"</span>), <span class="keyword">new</span> Path(<span class="string">"e:/file/test/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>HDFS文件夹删除</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 执行删除</span></span><br><span class="line">fs.delete(<span class="keyword">new</span> Path(<span class="string">"/HdfsDemo01/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>HDFS文件名更改</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 修改文件名称</span></span><br><span class="line">fs.rename(<span class="keyword">new</span> Path(<span class="string">"/HdfsDemo02/test01/fsdong.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/fseast.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>HDFS 文件详情查看<br>查看文件名称、权限、长度、块信息</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取文件详情</span></span><br><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">LocatedFileStatus status = listFiles.next();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出详情</span></span><br><span class="line"><span class="comment">// 文件名称</span></span><br><span class="line">System.out.println(status.getPath().getName());</span><br><span class="line"><span class="comment">// 长度</span></span><br><span class="line">System.out.println(status.getLen());</span><br><span class="line"><span class="comment">// 权限</span></span><br><span class="line">System.out.println(status.getPermission());</span><br><span class="line"><span class="comment">// 分组</span></span><br><span class="line">System.out.println(status.getGroup());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取存储的块信息</span></span><br><span class="line">BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">String[] hosts = blockLocation.getHosts();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">System.out.println(host);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>HDFS文件和文件夹判断</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件配置信息</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 判断是文件还是文件夹</span></span><br><span class="line">FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果是文件</span></span><br><span class="line"><span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">System.out.println(<span class="string">"f:"</span>+fileStatus.getPath().getName());</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">System.out.println(<span class="string">"d:"</span>+fileStatus.getPath().getName());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>传入一个路径，递归将该路径下的所有的文件还有目录打印到控制台</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">fs  = FileSystem.get(<span class="keyword">new</span> URI(uri), conf,user);</span><br><span class="line">printFileOrDir(<span class="string">"/"</span>, fs);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 传入一个路径 ，递归将该路径下的所有的文件还有目录打印到控制台</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printFileOrDir</span><span class="params">(String path , FileSystem fs)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(path));</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line"><span class="comment">//判断是文件还是目录</span></span><br><span class="line"><span class="keyword">if</span>(fileStatus.isFile()) &#123;</span><br><span class="line">System.out.println(<span class="string">"File:"</span> + path + <span class="string">"/"</span> + fileStatus.getPath().getName());</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// path:   hdfs://hadoop102:9000/0508</span></span><br><span class="line">String currentPath = fileStatus.getPath().toString().substring(<span class="string">"hdfs://hadoop102:9000"</span>.length());</span><br><span class="line"></span><br><span class="line"><span class="comment">//打印当前的目录</span></span><br><span class="line">System.out.println(<span class="string">"Dir:"</span> + currentPath);</span><br><span class="line"></span><br><span class="line"><span class="comment">//继续迭代当前目录下的子目录及文件</span></span><br><span class="line">printFileOrDir(currentPath, fs);</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-HDFS的I-O流操作"><a href="#3-HDFS的I-O流操作" class="headerlink" title="3.HDFS的I/O流操作"></a>3.HDFS的I/O流操作</h3><font color="#4169E1" size="4"> 1. HDFS文件上传 </font><br>需求：把本地E盘上 test02.txt 文件上传到HDFS根目录<br><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">putFileToHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 创建输入流</span></span><br><span class="line">FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"e:/test02.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 获取输出流</span></span><br><span class="line">FSDataOutputStream fos = fs.create(<span class="keyword">new</span> Path(<span class="string">"/test02.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 流对拷</span></span><br><span class="line">IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 关闭资源</span></span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><br><font color="#4169E1" size="4"> 2. HDFS文件下载 </font><br>需求：从HDFS上下载test02.txt文件到本地e盘上<br><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 文件下载</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileFromHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/text02.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 获取输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/test02..txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 流的对拷</span></span><br><span class="line">IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 关闭资源</span></span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><br><font color="#4169E1" size="4"> 3. 定位文件读取 </font><br>需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz（188M，大于128M所以上传到集群默认分成两块进行存储）<br><font color="#4169E1" size="3">（1）下载第一块： </font> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-2.7.2.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 创建输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/hadoop-2.7.2.tar.gz.part1"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 流的拷贝</span></span><br><span class="line"><span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span> ; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++)&#123;</span><br><span class="line">fis.read(buf);</span><br><span class="line">fos.write(buf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5关闭资源</span></span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><font color="#4169E1" size="3">（2）下载第二块<br><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"fseast"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 打开输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-2.7.2.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 定位输入数据位置</span></span><br><span class="line">fis.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 创建输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/hadoop-2.7.2.tar.gz.part2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 流的对拷</span></span><br><span class="line">IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 关闭资源</span></span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><br><font color="#4169E1" size="3"><br>（3）将下载到的两块文件合并查看是否完整<br>在Windows命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并：<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</span><br></pre></td></tr></table></figure><br><br>合并完成后，将hadoop-2.7.2.tar.gz.part1 重新命名为hadoop-2.7.2.tar.gz。解压发现该包是完整的。<br><br>## 四、HDFS的数据流<br>### 1.HDFS写数据流程<br>#### 2.1剖析文件写入<br><img src="https://img-blog.csdnimg.cn/20190728145944142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><font color="#4169E1" size="4">步骤分析： </font><p>（1）客户端通过 Distributed FileSystem 模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。<br>（2）NameNode返回是否可以上传。<br>（3）客户端请求第一个 Block 上传到那几个 DataNode 服务器上。<br>（4）NameNode 返回3个 DataNode节点（默认备份3份的情况下），分别为dn1，dn2，dn3,。<br>（5）客户端通过 FSDataOutputStream 模块请求dn1上传数据，dn1 收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。<br>（6）通道建立完成后，dn3应答dn2，dn2会答dn1，最后dn1应答客户端。<br>（7）客户端开始往dn1 上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（DataNode收到Packet会先存到本地磁盘，再把缓存中的Packet传到下一个DataNode）。全部存完从dn3开始往回应答。<br>（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block到服务器。重复执行3-7的步骤。<br>（9）传输完成以后，客户端会告诉NameNode数据传输完成。</p><h4 id="2-2网络拓扑-节点距离计算"><a href="#2-2网络拓扑-节点距离计算" class="headerlink" title="2.2网络拓扑 - 节点距离计算"></a>2.2网络拓扑 - 节点距离计算</h4><p>在HDFS写数据的过程中，NameNode会选择距离带上传数据最近距离的DataNode接受数据。</p><p><font color="#4169E1" size="4">最近距离如何计算</font><br>节点距离：两个节点到达最近的共同祖先的距离总和。看图更清晰：<br>假设有数据中心d1机架r1中的节点n1.该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。<br><img src="https://img-blog.csdnimg.cn/20190728204453690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="2-3Hadoop2-7-2-副本节点选择"><a href="#2-3Hadoop2-7-2-副本节点选择" class="headerlink" title="2.3Hadoop2.7.2 副本节点选择"></a>2.3Hadoop2.7.2 副本节点选择</h4><p>默认三个副本情况下的副本节点选择：<br>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。<br>第二个副本和第一个副本位于相同机架，随机节点。<br>第三个副本位于不同机架，随机节点。</p><h3 id="2-HDFS读数据流程"><a href="#2-HDFS读数据流程" class="headerlink" title="2.HDFS读数据流程"></a>2.HDFS读数据流程</h3><p><img src="https://img-blog.csdnimg.cn/20190728225310129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><font color="#4169E1" size="4">步骤分析:</font><br>（1）客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查找元数据，找到文件块所在的DataNode地址。<br>（2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。<br>（3）DataNode开始传输数据给客户端（从磁盘读取数据输入流，以Packet）为单位来做校验。<br>（4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。<br>（5）然后接着读取 第二个Block。</p></font></font>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、HDFS概述&quot;&gt;&lt;a href=&quot;#一、HDFS概述&quot; class=&quot;headerlink&quot; title=&quot;一、HDFS概述&quot;&gt;&lt;/a&gt;一、HDFS概述&lt;/h2&gt;&lt;p&gt;HDFS(Hadoop distributed File System)，它是一个文件系统，
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>阿里云服务器上的Hadoop伪分布式和完全分布式的搭建</title>
    <link href="http://yoursite.com/2018/02/23/buy-%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%92%8C%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/02/23/buy-阿里云服务器上的Hadoop伪分布式和完全分布式的搭建/</id>
    <published>2018-02-23T06:19:34.000Z</published>
    <updated>2019-09-11T00:30:26.255Z</updated>
    
    <content type="html"><![CDATA[<h2 id="搭建"><a href="#搭建" class="headerlink" title="搭建"></a>搭建</h2><p>搞来了三台阿里云服务器：<br>镜像：都是CentOS 7<br>JDK版本是1.8，<br>Hadoop版本是2.7.2，与上一篇虚拟机搭建用的软件包一致。</p><p><strong>一、集群规划</strong><br>hadoop101：NameNode，SecondaryNameNode，DataNode，NodeManager<br>hadoop102：DataNode，NodeManager<br>hadoop103：ResourceManager，DataNode，NodeManager</p><p><strong>二、搭建：</strong><br>基本步骤与上一篇一样，配置文件需要配置的内容也都一样。这里只说使用阿里云服务器搭建与虚拟机不同的地方。</p><p><strong>问题一：</strong><br>按照上篇文章配置方式配置完了以后，启动NameNode的时候起不来。<br>发现在/etc/hosts文件配置除了需要配置外网映射还要配置NameNode节点的内网，否则NameNode起不来，所以ResourceManager所安装的节点也需要配置该主机的内网。</p><p>hadoop101的hosts文件配置地址：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">47.100.xx.xxx hadoop101 #hadoop101外网地址映射</span><br><span class="line">120.79.xxx.xxx hadoop102</span><br><span class="line">39.108.xxx.xxx hadoop103</span><br><span class="line">172.19.xxx.xxx hadoop101  #hadoop101内网地址映射</span><br></pre></td></tr></table></figure><p>可以配内网地址也可以配127.0.0.1 hadoop101<br>然后就可以顺利启动整个集群了。</p><p><strong>问题二：</strong><br>关闭了防火墙，然后在hadoop101上传文件时，还是说通过50010端口数据传不过去：<br>INFO hdfs.DFSClient: Exception in createBlockOutputStream和<br>java.io.IOException: Got error, status message , ack with firstBadLink as </p><p><img src="https://img-blog.csdnimg.cn/20190719084716612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这个时候web查看文件：只有hadoop101节点上保存了块，其他两个节点并没有。<br><img src="https://img-blog.csdnimg.cn/20190719124823884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>解决问题二：</strong><br>在Linux已经确实关闭防火墙了，但却还是说端口过不去，然后就发现阿里云服务器除了在Linux系统关闭防火墙，还需要在阿里云控制台开发端口：<br>既然说了50010端口过不去，那就给它开个50010端口：<br>这是hadoop101阿里云ECS服务器控制台截屏：<br><img src="https://img-blog.csdnimg.cn/20190719125508744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>这是hadoop103阿里云轻量级服务器控制台截屏：<br><img src="https://img-blog.csdnimg.cn/20190719125245350.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">hadoop102并不需要在控制台操作，只需要在linux系统关闭防火墙即可，但其他两台不仅要在linux关闭，还要在控制台关闭。三台服务器都是不同时期买的，为什么会出现这样的差异情况也不清楚。</p><hr><p>分割线</p><hr><p>操作到这一步，开始上传文件，在NameNode节点也就是hadoop101上上传文件时，一切正常：<br><img src="https://img-blog.csdnimg.cn/20190719125940152.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190719130425764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><hr><p>分割线</p><hr><p><strong>问题三：</strong><br>但是在hadoop102和hadoop103上上传文件时，就出现问题了：</p><p>java.io.IOException: Got error, status message , ack with firstBadLink as 172.19.xxx.xxx:50010</p><p>这个172.19.xxx.xxx为hadoop101的内网ip。大概意思应该是尝试通过内网IP的50010端口传入数据失败。令人头疼的事情就出现了：在hadoop101节点hosts文件配内网ip地址的话，传数据会从这个内网传入，如果不配内网ip的话，NameNode启动不了。<br><img src="https://img-blog.csdnimg.cn/20190719112345813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">然后在web页面查看：<br>只有hadoop102和hadoop103上有存储块，hadoop101节点上竟然没有：</p><p><img src="https://img-blog.csdnimg.cn/20190719130810286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">所以就可以才猜测到，NameNode在内部使用的是内网地址，具体细节不太清楚。现在这个问题还没有解决。（希望知道如何处理的相互交流一下。）<br>在往后对集群的操作只能先在hadoop101上操作。</p><hr><p>分割线</p><hr><p>我试了一下在阿里云服务器控制台单独删了hadoop103开放的50010端口之后。<br>在hadoop101上传文件就得到下面的情况：</p><p>19/07/19 12:32:34 INFO hdfs.DFSClient: Exception in createBlockOutputStream<br>java.io.IOException: Got error, status message , ack with firstBadLink as  39.108.xxx.xxx:50010</p><p><img src="https://img-blog.csdnimg.cn/20190719123625511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20190719123709916.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;搭建&quot;&gt;&lt;a href=&quot;#搭建&quot; class=&quot;headerlink&quot; title=&quot;搭建&quot;&gt;&lt;/a&gt;搭建&lt;/h2&gt;&lt;p&gt;搞来了三台阿里云服务器：&lt;br&gt;镜像：都是CentOS 7&lt;br&gt;JDK版本是1.8，&lt;br&gt;Hadoop版本是2.7.2，与上一篇虚拟机搭
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>虚拟机上的Hadoop伪分布式和完全分布式的搭建</title>
    <link href="http://yoursite.com/2018/02/23/buy-%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8A%E7%9A%84Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%92%8C%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/02/23/buy-虚拟机上的Hadoop伪分布式和完全分布式的搭建/</id>
    <published>2018-02-23T01:10:11.000Z</published>
    <updated>2019-09-10T17:55:55.609Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Hadoop"><a href="#一、Hadoop" class="headerlink" title="一、Hadoop"></a>一、Hadoop</h2><h3 id="1-Hadoop的组成"><a href="#1-Hadoop的组成" class="headerlink" title="1.Hadoop的组成"></a>1.Hadoop的组成</h3><p>简单了解一下Hadoop2.x时代的组成：<br>HDFS  负责数据存储<br>Yarn  负责资源调度<br>MapReduce  负责计算<br>Common    辅助工具</p><h4 id="1-1HDFS架构概述"><a href="#1-1HDFS架构概述" class="headerlink" title="1.1HDFS架构概述"></a>1.1HDFS架构概述</h4><p>HDFS(Hadoop Distributed File System)<br>（1）NameNode (nn)：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的快列表和块所在的DataNode等。<br>（2）DataNode (dn)：在本地文件系统存储文件块数据，以及块数据的校验和。<br>（3）Secondary NameNode (2nn)：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。</p><h4 id="1-2YARN架构概述"><a href="#1-2YARN架构概述" class="headerlink" title="1.2YARN架构概述"></a>1.2YARN架构概述</h4><ol><li>ResourceManager（RM）主要作用如下：<br>（1）处理客户端请求<br>（2）监控NodeManager<br>（3）启动或监控ApplicationMaster<br>（4）资源的分配与调度</li><li>NodeManger（NM）主要作用如下：<br>（1）管理单个节点上的资源<br>（2）处理来自ResourceManager的命令<br>（3）处理来自ApplicationMaster的命令</li><li>ApplicationMaster（AM）作用如下：<br>（1）负责数据的切分<br>（2）为应用程序申请资源并分配给内部的任务。<br>（3）任务的监控与容错。</li><li>Container<br>Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。<br><img src="https://img-blog.csdnimg.cn/20190822152546302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li></ol><h4 id="1-3MapReduce架构概述"><a href="#1-3MapReduce架构概述" class="headerlink" title="1.3MapReduce架构概述"></a>1.3MapReduce架构概述</h4><p>MapReduce 将计算过程分为两个阶段：Map和Reduce：<br>（1）Map阶段并行处理输入数据。<br>（2）Reduce阶段对Map结果进行汇总</p><h2 id="二、Hadoop的搭建"><a href="#二、Hadoop的搭建" class="headerlink" title="二、Hadoop的搭建"></a>二、Hadoop的搭建</h2><h3 id="1-运行环境"><a href="#1-运行环境" class="headerlink" title="1.运行环境"></a>1.运行环境</h3><p>前期准备这一部分看需求进行配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改 vim /etc/udev/rules.d/70-persistent-net.rules , 拷贝mac地址</span><br><span class="line">修改 vim /etc/sysconfig/network-scripts/ifcfg-eth0 , 修改mac地址以及IP地址</span><br><span class="line">修改 vim /etc/sysconfig/network  修改主机名</span><br><span class="line">修改 vim /etc/hosts ,配置 IP与主机名的映射.</span><br></pre></td></tr></table></figure></p><ol><li>修改主机名</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p>配置IP与主机名的映射：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p>添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.17.101 hadoop101</span><br><span class="line">192.168.17.102 hadoop102</span><br><span class="line">192.168.17.103 hadoop103</span><br><span class="line">192.168.17.104 hadoop104</span><br></pre></td></tr></table></figure></p><ol><li>关闭防火墙<br>查看防火墙状态：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables status</span><br></pre></td></tr></table></figure></li></ol><p>临时关闭防火墙：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>开机时关闭防火墙：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><ol><li><p>创建Linux用户<br>这里添加了名为 fseast 的新用户：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">useradd fseast</span><br><span class="line">passwd fseast</span><br></pre></td></tr></table></figure></li><li><p>配置Linux用户具有root权限<br>对/etc/sudoers文件添加：</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fseast ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>接下来的操作都将使用fseast用户操作</p><ol><li>创建文件夹<br>在/opt下创建 software 和 module 两个目录，一个存放软件包，一个放解压后的文件。（使用fseast用户创建要使用sudo）<br>改变这两个目录所有者：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown fseast:fseast 目录</span><br></pre></td></tr></table></figure></li></ol><p>关闭图形化界面：<br>修改 /etc/inittab<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">id:3:initdefault:</span><br></pre></td></tr></table></figure></p><h4 id="1-1安装JDK"><a href="#1-1安装JDK" class="headerlink" title="1.1安装JDK"></a>1.1安装JDK</h4><p>这里使用的Linux版本是Centos6.8，<br>JDK版本是1.8，<br>Hadoop版本是2.7.2</p><ol><li>安装JDK<br>先把软件包上传到software 目录<br>解压：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li></ol><p>配置环境变量：<br>在/etc/profile文件加上：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure></p><p>使其生效：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></p><p>测试jdk是否安装成功：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></p><h4 id="1-2安装Hadoop"><a href="#1-2安装Hadoop" class="headerlink" title="1.2安装Hadoop"></a>1.2安装Hadoop</h4><p>Hadoop下载地址：<br><a href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/" target="_blank" rel="noopener">https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/</a></p><ol><li>上传安装包到software，解压：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li></ol><p>解压后查看目录结构：<br><img src="https://img-blog.csdnimg.cn/20190716190919341.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本<br>（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件<br>（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）<br>（4）sbin目录：存放启动或停止Hadoop相关服务的脚本<br>（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例</p><ol><li>将Hadoop添加到环境变量：<br>在/etc/profile文件添加：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure></li></ol><p>使其生效：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></p><p>测试是否安装成功：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure></p><h3 id="2-伪分布式运行模式"><a href="#2-伪分布式运行模式" class="headerlink" title="2.伪分布式运行模式"></a>2.伪分布式运行模式</h3><h4 id="2-1配置文件说明"><a href="#2-1配置文件说明" class="headerlink" title="2.1配置文件说明"></a>2.1配置文件说明</h4><p>Hadoop配置文件分为两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。<br>（1）默认配置文件：<br><img src="https://img-blog.csdnimg.cn/20190720204355548.png" alt="在这里插入图片描述"><br>（2）自定义配置文件:<br>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。</p><h4 id="2-2启动HDFS并运行MapReduce程序"><a href="#2-2启动HDFS并运行MapReduce程序" class="headerlink" title="2.2启动HDFS并运行MapReduce程序"></a>2.2启动HDFS并运行MapReduce程序</h4><p>(1). 修改配置文件：<br>进入<code>/opt/module/hadoop-2.7.2/etc/hadoop</code> 目录</p><font color="DeepSkyBlue" size="3">（a）配置：hadoop-env.sh</font><br>修改改配置文件的JAVA_HOME路径（其实单台节点不配JAVA_HOME也可以读的到该变量）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><br><br><font color="DeepSkyBlue" size="3">（b）配置：core-site.xml</font><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop101:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br><font color="DeepSkyBlue" size="3">（c）配置：hdfs-site.xml</font><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br>(2). 启动集群<br>（a）格式化NameNode（第一次启动时格式化，以后就不要总格式化，原因下面说）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><br><br>（b）启动NameNode<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><br><br>（c）启动DataNode<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure><br><br>(3). 查看集群<br>（a）查看是否启动成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop101 hadoop-2.7.2]$ jps</span><br><span class="line">5203 DataNode</span><br><span class="line">5353 Jps</span><br><span class="line">5102 NameNode</span><br></pre></td></tr></table></figure><br><br>（b）web端查看HDFS文件系统<br><a href="http://hadoop101:50070/" target="_blank" rel="noopener">http://hadoop101:50070/</a><br>使用hadoop101的话需要配置Windows的hosts文件。<br>成功进入：<br><img src="https://img-blog.csdnimg.cn/20190716204830465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">（c）查看产生的Log日志<br>要习惯根据日志提示信息去分析问题、解决Bug。<br>这里日志文件目录为：/opt/module/hadoop-2.7.2/logs<br><br>(4).  操作集群：<br>（a）在HDFS文件系统上创建一个input文件夹<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/fseast/input</span><br></pre></td></tr></table></figure><br><br>（b）将测试文件内容上传到文件系统上<br>先在本地创建一个文件wc.input，并写入一些单词，然后上传到文件系统上：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put wc.input /user/fseast/input</span><br></pre></td></tr></table></figure><br><br>（c）运行MapReduce程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/fseast/input/ /user/fseast/output</span><br></pre></td></tr></table></figure><br><br>这个 /user/fseast/output目录不用提前在HDFS创建。<br><br>（d）查看输出结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cat /user/fseast/output/*</span><br></pre></td></tr></table></figure><br><br>浏览器查看：<br><img src="https://img-blog.csdnimg.cn/20190716212843546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><br><strong>【为什么不能重复格式化NameNode？】</strong><br>Hadoop的NameNode和DataNode有对应的clusterID，NameNode的cID在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current/VERSION文件中，DataNode的cID在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/VERSION文件中，正常情况下这NameNode和DataNode的cID要一致。当重复格式化NameNode的时候，会导致NameNode的clusterID与DataNode的clusterID不一致。启动的时候便会出现问题。<br>所以，以后一定要格式化的时候，先关闭进程，删除/opt/module/hadoop-2.7.2下的data和logs这两个目录。<br>我截了NameNode的clusterID与DataNode的clusterID：<br><img src="https://img-blog.csdnimg.cn/20190716203810635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20190716204001540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>#### 2.3启动YARN并运行MapReduce程序<br>（1）配置集群<br><font color="DeepSkyBlue" size="3">（a）配置yarn-env.sh</font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><br><br><font color="DeepSkyBlue" size="3">（b）配置yarn-site.xml</font><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><br><br><font color="DeepSkyBlue" size="3">（c）配置：mapred-env.sh</font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><br><br><font color="DeepSkyBlue" size="3">（d）配置： (对mapred-site.xml.template复制一份并重新命名为) mapred-site.xml</font><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在YARN上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>切记上面的配置都要在 <code>&lt;configuration&gt;&lt;/configuration&gt;</code>  内</p><p>（2）启动集群<br>（a）启动前必须保证NameNode和DataNode已经启动<br>（b）启动ResourceManager<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure></p><p>（c）启动NodeManager<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure></p><p>截图：<br><img src="https://img-blog.csdnimg.cn/20190717163826446.png" alt="在这里插入图片描述"><br>（3）集群操作<br>（a）YARN的浏览器页面查看：<a href="http://hadoop101:8088" target="_blank" rel="noopener">http://hadoop101:8088</a><br>如图所示：<br><img src="https://img-blog.csdnimg.cn/20190717164054991.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">（b）删除文件系统上的output文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -rm -R /user/fseast/output</span><br></pre></td></tr></table></figure></p><p>（c）执行MapReduce程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/fseast/input /user/fseast/output</span><br></pre></td></tr></table></figure></p><p>执行MapReduce程序的时候，如果你一直刷新页面，就可以看的到变化：<br><img src="https://img-blog.csdnimg.cn/20190717165519297.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20190717165554655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="2-4配置历史服务器"><a href="#2-4配置历史服务器" class="headerlink" title="2.4配置历史服务器"></a>2.4配置历史服务器</h4><p>为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：</p><ol><li><p><font color="DeepSkyBlue" size="3">配置mapred-site.xml:</font></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动历史服务器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></li><li><p>查看历史服务器是否启动：<br><img src="https://img-blog.csdnimg.cn/20190717181121553.png" alt="hadoop历史服务器"></p></li><li><p>查看JobHistory：<a href="http://hadoop101:19888/jobhistory" target="_blank" rel="noopener">http://hadoop101:19888/jobhistory</a><br>如图所示：<br><img src="https://img-blog.csdnimg.cn/20190717181700122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">点击上方圈起来的位置：<br><img src="https://img-blog.csdnimg.cn/20190717181837376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">再点击圈起来的地方：<br><img src="https://img-blog.csdnimg.cn/20190717181913114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">他说没有开启聚集，那就开启一下日志的聚集：</p></li></ol><h4 id="2-5配置日志的聚集"><a href="#2-5配置日志的聚集" class="headerlink" title="2.5配置日志的聚集"></a>2.5配置日志的聚集</h4><ol><li><font color="DeepSkyBlue" size="3">配置yarn-site.xml:</font></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li><p>关闭NodeManager 、ResourceManager和HistoryServer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop101 hadoop]$ yarn-daemon.sh stop resourcemanager</span><br><span class="line">[fseast@hadoop101 hadoop]$ yarn-daemon.sh stop nodemanager</span><br><span class="line">[fseast@hadoop101 hadoop]$ mr-jobhistory-daemon.sh stop historyserver</span><br></pre></td></tr></table></figure></li><li><p>启动NodeManager 、ResourceManager和HistoryServer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop101 hadoop]$ yarn-daemon.sh start resourcemanager</span><br><span class="line">[fseast@hadoop101 hadoop]$ yarn-daemon.sh start nodemanager</span><br><span class="line">[fseast@hadoop101 hadoop]$ mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></li><li><p>删除HDFS上已经存在的输出文件</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -rm -R /user/fseast/output</span><br></pre></td></tr></table></figure><ol><li>执行WordCount程序</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/fseast/input /user/fseast/output</span><br></pre></td></tr></table></figure><p>再按照上面，查看日志，<br>先进JobHistory，<a href="http://hadoop101:19888/jobhistory" target="_blank" rel="noopener">http://hadoop101:19888/jobhistory</a><br><img src="https://img-blog.csdnimg.cn/20190717183938407.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20190717184001971.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20190717184115874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20190717184200523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="3-完全分布式运行模式"><a href="#3-完全分布式运行模式" class="headerlink" title="3.完全分布式运行模式"></a>3.完全分布式运行模式</h3><h4 id="3-1虚拟机准备"><a href="#3-1虚拟机准备" class="headerlink" title="3.1虚拟机准备"></a>3.1虚拟机准备</h4><p>再准备三台虚拟机：hadoop102、hadoop103、hadoop104，修改主机名，IP地址，配置/etc/hosts文件，</p><h4 id="3-2编写集群分发脚本"><a href="#3-2编写集群分发脚本" class="headerlink" title="3.2编写集群分发脚本"></a>3.2编写集群分发脚本</h4><ol><li>scp（secure copy）安全拷贝<br>把上面安装好的jdk和Hadoop发送到新建的三台虚拟机(记得先停掉hadoop的那些进程)：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop101 opt]$ scp -r /opt/module root@hadoop102:/opt/</span><br><span class="line">[fseast@hadoop101 opt]$ scp -r /opt/module root@hadoop103:/opt/</span><br><span class="line">[fseast@hadoop101 opt]$ scp -r /opt/module root@hadoop104:/opt/</span><br></pre></td></tr></table></figure><p>改变传过去目录的所有者：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 opt]$ sudo chown fseast:fseast -R module/</span><br></pre></td></tr></table></figure></p><ol><li>rsync 远程同步工具<br>rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。<br>rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。</li></ol><p>实例：<br>把hadoop101机器上的/opt/software目录同步到hadoop102服务器的root用户下的/opt/目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop101 opt]$ sudo rsync -av /opt/software/ hadoop102:/opt/software</span><br></pre></td></tr></table></figure></p><p>拷贝环境变量配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop101 etc]$ sudo rsync -av /etc/profile hadoop102:/etc/profile</span><br></pre></td></tr></table></figure><p>使环境变量生效：<code>source /etc/profile</code></p><p><strong>脚本实现：</strong><br>目的：后面在hadoop102节点上修改了某些文件时，不需要一个个传到另外两个节点，启动 shell 脚本时加上参数即可：<br>（a）在/home/fseast目录下创建bin目录，并在bin目录下xsync创建文件，文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 ~]$ mkdir bin</span><br><span class="line">[fseast@hadoop102 ~]$ cd bin/</span><br><span class="line">[fseast@hadoop102 bin]$ touch xsync</span><br><span class="line">[fseast@hadoop102 bin]$ vim xsync</span><br></pre></td></tr></table></figure><p>在该文件中编写如下代码：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line"><span class="meta">#</span>1 获取输入参数个数，如果没有参数，直接退出</span><br><span class="line">pcount=$#</span><br><span class="line">if ((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>2 获取文件名称</span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>3 获取上级目录到绝对路径</span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>4 获取当前用户名称</span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>5 循环</span><br><span class="line">for((host=103; host&lt;105; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -av $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>（b）修改脚本 xsync 具有执行权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 bin]$ chmod 777 xsync</span><br></pre></td></tr></table></figure><p>（c）调用脚本形式：xsync 文件名称<br>如：<br>把/home/fseast/bin同步到其他两台节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 bin]$ xsync /home/fseast/bin</span><br></pre></td></tr></table></figure><p>注意：如果将xsync放到/home/fseast/bin目录下仍然不能实现全局使用，可以将xsync移动到/usr/local/bin目录下。出现不能使用的情况，大多是全局变量PATH没有/home/fseast/bin路径。</p><h4 id="3-3集群配置"><a href="#3-3集群配置" class="headerlink" title="3.3集群配置"></a>3.3集群配置</h4><p><font color="Red" size="3">以下的完全分布式配置是完整的配置，也就是默认没有配置伪分布式情况下的。</font></p><p><table><tr align="left"><td bgcolor="#F0FFF0">配置文件三个.env结尾的文件都只是配了 JAVA_HOME ，所以也可不配，只需要在/home/fseast/.bashrc文件中加上 source /etc/profile</td></tr></table><br>NameNode，ResourceManager，SecondaryNameNode三个节点比较耗资源，最好不要放在同一台机器。</p><ol><li>集群部署规划<br><img src="https://img-blog.csdnimg.cn/20190717203826723.png" alt="在这里插入图片描述"></li><li>SSH免密登录配置<br>(1) 生成公钥和私钥：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 ~]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure></li></ol><p>然后按三次回车，就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p><p>(2) 将公钥拷贝到要免密登录的目标机器上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">[fseast@hadoop102 .ssh]$ ssh-copy-id hadoop103</span><br><span class="line">[fseast@hadoop102 .ssh]$ ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure></p><p>在hadoop103也要做相同操作，这里hadoop104可以操作也可以不做。</p><ol><li>配置集群<br>配置集群的文件在hadoop102节点配置，配置完后再使用上面的脚本同步就好。<br>这里是按照没有配伪分布式情况下的配置文件，在前面配过伪分布式那么有些配过了那就不需要重复配了。<br>所用需要配置的文件都在目录：<br>/opt/module/hadoop-2.7.2/etc/hadoop/slaves</li></ol><p>（1）核心配置文件</p><p><font color="DeepSkyBlue" size="3">配置core-site.xml</font>（伪分布式配过，只需要修改NameNode的节点名即可。）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（2）HDFS配置文件</p><p><font color="DeepSkyBlue" size="3">配置hadoop-env.sh</font>（伪分布式配过）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p><font color="DeepSkyBlue" size="3">配置hdfs-site.xml</font>（副本数量伪分布式配过，不过需要修改）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）YARN配置文件</p><p><font color="DeepSkyBlue" size="3">配置yarn-env.sh</font>（伪分布式配过）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p><font color="DeepSkyBlue" size="3">配置yarn-site.xml</font>（伪分布式配过，需要修改ResourceManager的地址，前面配的日志聚集也可保留）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）MapReduce配置文件</p><p><font color="DeepSkyBlue" size="3">配置mapred-env.sh</font>（伪分布式配过）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p><font color="DeepSkyBlue" size="3">配置mapred-site.xml</font>（伪分布配过，没配过的需要复制mapred-site.xml.template文件并改名为mapred-site.xml再配置）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定MR运行在Yarn上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（5）<font color="DeepSkyBlue" size="3">配置slaves</font>（没有配过）：<br>为了群起集群的时候，知道哪台节点是从节点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure></p><ol><li>在集群上分发配置好的Hadoop配置文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[fseast@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/</span><br></pre></td></tr></table></figure><ol><li>群起集群<br>（1）如果集群是第一次启动，需要格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure></li></ol><p>（2）启动HDFS<br>在hadoop102（NameNode）执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure></p><p>（3）启动YARN<br>在hadoop103（ResourceManager）执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>（4）Web端查看SecondaryNameNode：<br><a href="http://hadoop104:50090" target="_blank" rel="noopener">http://hadoop104:50090</a></p><p><img src="https://img-blog.csdnimg.cn/20190717225014645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="集群启动-停止方式总结"><a href="#集群启动-停止方式总结" class="headerlink" title="集群启动/停止方式总结"></a>集群启动/停止方式总结</h5><ol><li><p>各个服务组件逐一启动/停止<br> （1）分别启动/停止HDFS组件</p><pre><code>hadoop-daemon.sh  start / stop   namenode / datanode / secondarynamenodehadoop-daemon.sh  start / stop   datanode hadoop-daemon.sh  start / stop   secondarynamenode</code></pre><p> （2）启动/停止YARN</p><pre><code>yarn-daemon.sh  start / stop  resourcemanageryarn-daemon.sh  start / stop  nodemanager</code></pre></li><li><p>各个模块分开启动/停止（配置ssh是前提）<br> （1）整体启动/停止HDFS</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh   /  stop-dfs.sh</span><br></pre></td></tr></table></figure><p>（2）整体启动/停止YARN<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh  /  stop-yarn.sh</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、Hadoop&quot;&gt;&lt;a href=&quot;#一、Hadoop&quot; class=&quot;headerlink&quot; title=&quot;一、Hadoop&quot;&gt;&lt;/a&gt;一、Hadoop&lt;/h2&gt;&lt;h3 id=&quot;1-Hadoop的组成&quot;&gt;&lt;a href=&quot;#1-Hadoop的组成&quot; class
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
  </entry>
  
</feed>
